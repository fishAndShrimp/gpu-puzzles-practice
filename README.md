# âš¡ GPU Puzzles: Solutions & Notes

Huge thanks to the author of [GPU Puzzles](https://github.com/srush/GPU-Puzzles). Unlike [Tensor Puzzles](https://github.com/srush/Tensor-Puzzles), which focuses on one-line algorithmic logic, **GPU Puzzles** feels much more like a hands-on CUDA programming exercise. The Colab environment is convenient, and the visualization tools showing data flow **make it intuitive to understand** the CUDA Memory Model.

éå¸¸æ„Ÿè°¢ [GPU Puzzles](https://github.com/srush/GPU-Puzzles) çš„ä½œè€…ã€‚ä¸ä¾§é‡â€œä¸€è¡Œä»£ç é€»è¾‘è§£è°œâ€çš„ [Tensor Puzzles](https://github.com/srush/Tensor-Puzzles) ä¸åŒï¼Œ**GPU Puzzles** æ›´åå‘ Hands-on çš„ CUDA ç¼–ç¨‹å®æˆ˜ã€‚Colab ç¯å¢ƒéå¸¸æ–¹ä¾¿ï¼Œç‰¹åˆ«æ˜¯é‚£ä¸ªæŒ‡ç¤ºæ•°æ®æµå‘çš„å¯è§†åŒ–å›¾è¡¨ï¼Œè®©ç†è§£ **CUDA å†…å­˜æ¨¡å‹**ï¼ˆå°¤å…¶æ˜¯ Global ä¸ Shared Memory çš„äº¤äº’ï¼‰å˜å¾—éå¸¸ç›´è§‚ã€‚

---

## ğŸ’¡ Core Concepts / æ ¸å¿ƒä½“ä¼š

I highly recommend reading **PMPP** (*Programming Massively Parallel Processors*) alongside these puzzles to grasp the underlying hardware concepts. Initially, I was confused: *Why does each thread handle only one point? Shouldn't we use loops and strides?*

å»ºè®®é…åˆ **PMPP** (*Programming Massively Parallel Processors*) é˜…è¯»ä»¥è·å¾—æ¦‚å¿µæ€§çš„æ”¯æŒã€‚èµ·åˆæˆ‘å¾ˆç–‘æƒ‘ï¼šâ€œä¸ºä»€ä¹ˆæ¯ä¸ª Thread åªå¤„ç†ä¸€ä¸ªç‚¹ï¼Œè€Œä¸ç”¨å¾ªç¯ Strideï¼Ÿâ€

The book explains that **GPU threads are extremely lightweight**. Unlike CPU threads, creating thousands of them is cheap. Apart from the warps actually executing on the SM, the queued threads have virtually no overhead. Itâ€™s less about "looping" and more about "massive parallelism."

ä¹¦ä¸­è§£é‡Šé“ï¼Œ**GPU çº¿ç¨‹æå…¶å»‰ä»· (Lightweight)**ã€‚ä¸ CPU çº¿ç¨‹ä¸åŒï¼Œåˆ›å»ºæˆåƒä¸Šä¸‡ä¸ª GPU çº¿ç¨‹æˆæœ¬å¾ˆä½ã€‚é™¤äº†æ­£åœ¨ SM ä¸Šæ‰§è¡Œçš„ Warpï¼Œæ’é˜Ÿä¸­çš„çº¿ç¨‹å‡ ä¹æ²¡æœ‰é¢å¤–å¼€é”€ã€‚GPU çš„è®¾è®¡å“²å­¦ä¸æ˜¯â€œå¾ªç¯å¤„ç†â€ï¼Œè€Œæ˜¯â€œæµ·é‡çº¿ç¨‹å¹¶è¡Œâ€ã€‚

---

## ğŸ“ Key Takeaways / å…³é”®ç¬”è®°

### Puzzle 11: Conv 1D (Memory Load Balancing)
The key insight here is **Load Balancing** when moving data to Shared Memory. By carefully distributing the reading tasks among threads, we reduced the maximum global reads per thread (e.g., from 3 to 2). This optimization effectively helps **hide memory latency**.

è¿™é‡Œçš„å…³é”®åœ¨äºæ¬è¿æ•°æ®åˆ° Shared Memory æ—¶çš„**è´Ÿè½½å‡è¡¡**ã€‚é€šè¿‡åˆç†åˆ†é…è¯»å–ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªçº¿ç¨‹çš„æœ€å¤§ Global Read æ¬¡æ•°é™ä½ï¼ˆä¾‹å¦‚ä» 3 æ¬¡é™åˆ° 2 æ¬¡ï¼‰ã€‚è¿™ä¸ªä¼˜åŒ–èƒ½æœ‰æ•ˆå¸®åŠ©**æ©ç›–å†…å­˜å»¶è¿Ÿ**ã€‚

### Puzzle 12: Prefix Sum (Blelloch Algorithm)
This involves the **Blelloch Scan** algorithm. A great way to understand the `Downsweep` phase is to view a node's value as **"the prefix sum of the range preceding this node's jurisdiction."** By processing layer by layer down to the leaves, we ensure every operation maintains this property. Once the leaf nodes are reached, the prefix sum for every position naturally emerges.

æ¶‰åŠ **Blelloch ç®—æ³•**ã€‚ç†è§£ `Downsweep` è¿‡ç¨‹çš„ä¸€ä¸ªç›´è§‚è§†è§’æ˜¯ï¼šæŠŠå½“å‰ Node çš„å€¼ç†è§£ä¸º**â€œè¯¥ Node ç®¡è¾–èŒƒå›´ä¹‹å‰çš„ Prefix Sumâ€**ã€‚é€šè¿‡é€å±‚å¤„ç†ç›´åˆ°å¶å­èŠ‚ç‚¹ï¼Œæ¯ä¸€æ¬¡æ“ä½œéƒ½åœ¨ç»´æŠ¤è¿™ä¸ªæ€§è´¨ï¼Œå½“æ‰€æœ‰å¶å­èŠ‚ç‚¹å¤„ç†å®Œæ¯•åï¼Œè‡ªç„¶å°±å¾—åˆ°äº†æ¯ä¸ªä½ç½®çš„æ­£ç¡® Prefix Sumã€‚

### Puzzle 13: Axis Sum (Grid Mapping Strategy)
In this puzzle, `blockIdx.y` is mapped to the batch dimension. This reflects a strategy where `Global_Y` represents the flattened outer dimensions (e.g., `Batch * Seq` in a `(Batch, Seq, Hidden)` shape). By setting `Global_Y = blockIdx.y`, the Grid Y-axis handles independent rows naturally.

æœ¬é¢˜å°† `blockIdx.y` æ˜ å°„åˆ°äº† Batch ç»´åº¦ã€‚è¿™åæ˜ äº†ä¸€ç§ç­–ç•¥ï¼šç”¨ `Global_Y` æ¥è¡¨ç¤º Flatten åçš„å‰ç½®ç»´åº¦ï¼ˆä¾‹å¦‚ `(Batch, Seq, Hidden)` ä¸­çš„ `Batch * Seq`ï¼‰ã€‚é€šè¿‡ `Global_Y = blockIdx.y`ï¼Œè®© Grid çš„ Y è½´è‡ªç„¶åœ°å¤„ç†ç›¸äº’ç‹¬ç«‹çš„è¡Œã€‚
